\documentclass{article}

\title{Project Results: Validated Peephole Optimization for CompCert's x86 Backend}
\author{Andrew Sackville-West, Thomas M. DuBuisson}

\begin{document}
\maketitle

\section{Introduction}

Compcert's x86 back-end is known to produce inefficient code, even
when compared to ARM and PPC instructions produced from the same
source code.  Informal observation suggests there is opportunity for
performance improvement using an architecture specific peephole
optimization.

\section{Method}

In a similar vein to work by Tristian and Leroy's Translation
Validators, we have an initial implementation of a ML peephole
optimizer, a Gallina validator performing symbolic execution to
compare pre- and post-optimized code, a parent ``optimize'' routine to
traverse the instructions, and proofs of the validator's correctness.
Below we address each of these components individually.

\section{Symbolic-Execution Based Validator}

The validator accepts the pre- and post-optimized code as arguments.
It then symbolicly executes each of the pieces of code and compares
the result.  Symbolic execution results in three critical pieces of
information: the state of memory, the state of the registers
(including flags), and a set of ``constraints'' (operations that might
produce side effects not captured by the symbolic expressions, such as
dividing by zero).

In order to properly compare two symbolic expressions, it is important
to normalize them. Our validation is based on syntactic equivalence of
expressions. Many syntactically different expressions can be
semantically equivalent. Normalizing the symbolic expressions allows
some semantically equivalent expressions to be accepted by the
validator. Our normalization includes transformations which allow
instruction weakening, though that particular optimization is not
currently performed.

\section{Status of proofs: }
We have defined a set of inductive propositions which make explicit
the meaning of two symbolic states being equivalent. Generally
speaking, we use syntactic equality between symbolic expressions to
express equivalence between portions of symbolic state. But
in some cases, e.g., status flags, we allow different relations to
suffice. For example, in keeping with the convention on CompCert, if a
flag becomes {\it more defined} as a result of an operation, that flag
would be considered equivalent to the less defined version. 

The overall equivalence proposition is :

\begin{verbatim}
Inductive symStates_match : SymState -> SymState -> Prop :=
| symStates_match_intro : 
  forall s1 s2, 
    symAllFlags_match s1 s2 ->
    symAllRegs_match s1 s2 ->
    symMemory_match s1 s2 ->
    subset (constraints s1) (constraints s2) = true ->
    symStates_match s1 s2. 
  
\end{verbatim}

This proposition captures our definition of symbolic state
equivalence. To support this proposition, there are a number of
additional inductive propositions and correctness lemmas for various
boolean decision properties. 

The correctness of our optimizations depends on the validation routine
only accepting code fragments for which we can prove the above
proposition. This is stated in  the correctness theorem for our
validation routine:

\begin{verbatim}
  Theorem peephole_validate_correct : forall (c d : code) (s1 s2 : SymState),
  peephole_validate c d = true -> 
    symExec c initSymSt = Some s1 -> 
    symExec d initSymSt = Some s2 ->
    symStates_match s1 s2.
\end{verbatim}

This theorem has been completely proved for the model as it
exists.

\section{Optimizer (Gallina side - plumbing)}
The optimize function accepts a basic block and divides it into
smaller blocks, breaking the list of instructions at each instruction
that is not recognized by our symbolic execution.  For each of these
``executable blocks'', we optimize the code and validate the result
has the same execution as the original instructions.  If the
validation ever fails, the optimized block is discarded in favor of
the original.

\section{Optimizer (ML side - functionality}
The actual optimizations are load/store and removal of redundant loads
({\tt load reg mem; \{op;\}* load reg mem;} $\rightarrow$ {\tt load reg
  mem; \{op;\}*}).  The Gallina code provides the optimizer with the
largest possible block that can be symbolicly executed, the optimizer
is in charge of extracting suitable windows for the operation of
individual optimization rules.

\section{Dark Corners}

\subsection{Assumptions in the Existing Proofs}


\subsection{Aspects of Symbolic Execution}
\paragraph{Limited Normalization: }The validator's normalization
process translates all symbolic expressions into a normal form,
allowing direct comparison of expressions from differint executions.
Unfortunately, there exists a (unimplemented) need for mutual
recursion between the normalization of expressions and the
normalization of expressions stored in memory (the state of memory is
often captured as a component of symbolic expressions).  In lieu of
this mutual recursion, any two executions that are semantically the
same but symbolically different (ex: {\tt mult x 2} vs {\tt add x x})
will only be properly normalized and accepted if the non-normalized
value was kept in the registers and never {\it spilled} into memory.

Finally, there is no alias analysis used when normalizing loads into
the stored expressions.  This isn't a fundamental issue, merely a
failing due to time constraints and the complexity of comparing the
rather verbose {\tt SymExpr}'s for identical base and constant offset
of sufficient size.

\paragraph{Shortcomings: }
Certain instructions, particularly those working on data
smaller than 32 bits, have no symbolic execution.

\paragraph{Treatment of flags: } 
The flags follow x86 semantics~\footnote{x86 semantics as defined by
  Intel} as closely as possible~\footnote{CompCert ignores some flags
  and conflates others, providing only one register bit for two
  separate flags.}.  By following the Intel specification, the
validator is more restrictive than, but not in conflict with, the
CompCert semantics which declares the flags to be undefined after most
arithmetic instructions.  Because this work defines normalization as
an operation that maintains arithmetic equivalence, but not necesarily
the flag value's equivalence, it is not possible to normalize the
expressions when comparing flag values.  Instead, a straight forward
syntaxtic equality is used which could result in unnecessary rejection
of optimized code.

\section{False Starts}
In general, the false starts give a solid ``lessons learned'' of
explicitly identifying and verifying the assumptions early.

\paragraph{Direct addition to the x86 ASM stage: }
The first attempt was to add peephole optimization directly to the ASM
stage in CompCert.  After much wrestling with existing proofs, which
broke sometimes only incidentally due to the peephole optimizer, we
decided to instead add this work as a new final stage to the CompCert
pipeline.  As a result there were minor, high level, additions in
Compiler.v to both add the optimization pass and apply an (admitted)
theorem for the end-to-end proof.

\paragraph{Avoidance of Alias Analysis: }
Our canonical optimizations were instruction weakening and removing
sequential redundant loads, neither of which require alias analysis.
Due to pseudo instructions there were zero redundant loads visiable to
the peephole optimizer; coupled with the generally larger scope of
instruction weakening, this forced us to reconsider our intended
showcase optimization and include proper alias analysis.

\paragraph{Use of {\tt addrmode} for address information}
Even late into development CompCert's {\tt addrmode} was used to as the
memory addressing under the implicit assumption addrmode equality would 
correctly identify aliasing.  Briefly, {\tt addrmode}'s are
insufficient because they capture a base in terms of a particular register
and not the contents of the register at the time the address was used.
Replacing this dependence was the source of significant code and proof
reworking.


\section{Impact of the Optimizer}
In our test code (consisting of benchmarks from the ``Computer
Language Shoot-out'') we remove 29 assembly instructions when
compiling ~1000 lines of C code that result in 2416 assembly
instructions.  All the instruction removals are due to the redundant
load optimization.

\section{Conclusion}

\clearpage
\appendix
\section{Installation Instructions}
To obtain the code simply git clone the repository.  From here on we
assume you cloned into the {\tt ccomp-ph} directory.

\begin{verbatim}
$ git clone git://github.com/TomMD/CompCert-Peep-hole-Optimizer.git ccomp-ph
\end{verbatim}

The source can be configured, compiled, and installed in the normal
manner (we recommend using Coq 8.2pl2):

\begin{verbatim}
$ cd ccomp-ph/compcert-1.8
$ ./configure ia32-linux && make all && sudo make install
\end{verbatim}

Compiling and testing, be it generating assembly or building an
executable, can proceed as normal:

\begin{verbatim}
$ cd comp-ph/benchmarks/clso
$ ccomp -S *.c
$ ls *.s
\end{verbatim}

\section{Code Layout}
Within the {\tt ccomp-ph/compcert-1.8} directory the following files
were modified and/or added:

\begin{itemize}
\item {\tt ia32/Peephole.v} -- contains the symbolic execution,
  validator, and call-out to the ML optimizer.
\item {\tt ia32/Peepholeproof.v} -- contains proofs about
  properties of the operations in {\tt Peephole.v}.
\item {\tt ia32/Peepholeaux.ml\{,i\}} -- the ML peephole
  optimizer with basic, load and store centric, optimizations.
\item {\tt driver/Compiler.v} -- Modified to call our
  optimization pass and use the (admitted) proof of correctness.
\item {\tt ia32/Asm.v} -- added equality tests for various data
  types and added a {\tt NOP} instruction.
\item {\tt extraction/extraction.v} -- modified in the obvious
  manner to enable calling the ML optimizer.
\end{itemize}

\end{document}

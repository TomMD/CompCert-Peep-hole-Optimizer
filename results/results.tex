\documentclass{article}

\title{Project Results: Validated Peephole Optimization for CompCert's x86 Backend}
\author{Andrew Sackville-West, Thomas M. DuBuisson}

\begin{document}
\maketitle

\section{Introduction}

Compcert's x86 back-end is known to produce inefficient code, even
when compared to the ARM and PPC instructions produced from the same
source code.  Informal observation suggests there is opportunity for
performance improvement using an architecture specific peephole
optimization.

\section{Method}

Following the example of Tristan and Leroy, we use a technique called
translation validation to implement a peephole optimizer for the
CompCert x86 backend. In this implementation, an unverified optimizer
implemented in ML traverses small segment of assembly code making
local optimizations. A validation routine, implemented in the Coq
theorem prover, examines these optimizations using symbolic evaluation
and decision procedures to determine whether the optimizations are
correct. The validation routine is supported with proofs of it's
correctness.  Below we address each of these components individually.

\section{Symbolic-Execution Based Validator}

The validator accepts pre- and post-optimized code as arguments.
It then symbolicly executes each of the pieces of code and compares
the result.  Symbolic execution results in three critical pieces of
information: the state of memory, the state of the registers
(including flags), and a set of ``constraints'' (operations that might
produce side effects not captured by the symbolic expressions, such as
dividing by zero).

In order to properly compare the results of symbolic execution, it is
important to normalize the symbolic expressions produced. Our
validation is based on syntactic equivalence of expressions, but many
syntactically different expressions can be semantically
equivalent. Normalizing the symbolic expressions allows semantically
equivalent expressions to be put into a canonical form. Thus, certain
classes of semantically equivalent expressions which would otherwise
be rejected by the validator can instead be accepted. Our
normalization includes transformations which allow instruction
weakening, such as replacing an {\tt leal} instruction used for
multiplying by a power of two with an equivalent {\tt sal}
instruction, though that particular optimization is not currently
performed.

\section{Status of proofs}
We have defined a set of inductive propositions which make explicit
the meaning of two symbolic states being equivalent. Generally
speaking, we use syntactic equality between symbolic expressions to
express equivalence between portions of symbolic state. But
in some cases, e.g., status flags, we allow different relations to
suffice. For example, in keeping with the convention on CompCert, if a
flag becomes {\it more defined} as a result of an operation, that flag
would be considered equivalent to the less defined version. 

The overall equivalence proposition is :

\begin{verbatim}
Inductive symStates_match : SymState -> SymState -> Prop :=
| symStates_match_intro : 
  forall s1 s2, 
    symAllFlags_match s1 s2 ->
    symAllRegs_match s1 s2 ->
    symMemory_match s1 s2 ->
    subset (constraints s1) (constraints s2) = true ->
    symStates_match s1 s2. 
  
\end{verbatim}

This proposition captures our definition of symbolic state
equivalence. To support this proposition, there are a number of
additional inductive propositions and correctness lemmas for various
boolean decision properties. 

The correctness of our optimizations depends on the validation routine
only accepting code fragments for which we can prove the above
proposition. This is stated in  the correctness theorem for our
validation routine:

\begin{verbatim}
  Theorem peephole_validate_correct : forall (c d : code) (s1 s2 : SymState),
  peephole_validate c d = true -> 
    symExec c initSymSt = Some s1 -> 
    symExec d initSymSt = Some s2 ->
    symStates_match s1 s2.
\end{verbatim}

This theorem has been completely proved for the model as it
exists.

\section{Optimizer (Gallina side - plumbing)}
The optimize function accepts a basic block and divides it into
smaller blocks, breaking the list of instructions at each instruction
that is not recognized by our symbolic execution.  For each of these
``executable blocks'', we optimize the code and validate the result
using the verified validator.  If the validation ever fails, the
optimized block is discarded in favor of the original.

\section{Optimizer (ML side - functionality)}
The currently implemented optimizations are load/store and removal of
redundant loads ({\tt load reg mem; \{op;\}* load reg mem;}
$\rightarrow$ {\tt load reg mem; \{op;\}*}).  The Gallina code
provides the optimizer with the largest possible block that can be
symbolicly executed, the optimizer is in charge of extracting suitable
windows for the operation of individual optimization rules.

\section{Dark Corners}

\subsection{Shortcomings in the Existing Proofs}
Although we have developed what we believe to be a reasonable set of
inductive propositions defining equivalent symbolic states there are
several notable shortcomings. First, and perhaps most important, it is
not readily apparent that this definition can be related to the
CompCert proof of semantic equivalence between programs. This
shortcoming follows almost directly from our failure to properly
consider the CompCert semantics from the beginning. Although we did
initially develop backwards from the admitted theorem required to
complete the CompCert proof, that work was dropped in favor of
``getting something done.'' Ultimately, this caused our development to
diverge from the requirements needed to connect to the CompCert
framework properly.

Additionally, encoded in our inductive propositions is the assumption
that the normalization procedure is correct. One proposal is to move
away from the use of syntactic equality between symbolic expressions to
some inductive relation on symbolic expressions. This relation could
rely on a proof of semantic equivalence between normalized and
un-normalized  expressions. This would then require a proof of
correctness for the normalization procedure in order to express
equivalence of symbolic expressions, and, by extension, symbolic
states. This requires development of a denotation for our symbolic
expressions in the CompCert semantic framework.


\subsection{Aspects of Symbolic Execution}
\paragraph{Limited Normalization: }The validator's normalization
process translates all symbolic expressions into a normal form,
allowing direct comparison of expressions from different executions.
Unfortunately, there exists a (unimplemented) need for mutual
recursion between the normalization of expressions and the
normalization of expressions stored in memory (the state of memory is
often captured as a component of symbolic expressions).  In lieu of
this mutual recursion, any two executions that are semantically the
same but symbolically different (ex: {\tt mult x 2} vs {\tt add x x})
will only be properly normalized and accepted if the non-normalized
value was kept in the registers and never {\it spilled} into memory.

Finally, there is no alias analysis used when normalizing loads into
the stored expressions.  This isn't a fundamental issue, but rather a
failing due to time constraints and the complexity of extracting
appropriate base and offset data from the rather verbose {\tt
  SymExpr}'s used to represent addresses.

\paragraph{Unimplemented Instructions: }
Certain instructions, particularly those working on data smaller than
32 bits, have no symbolic execution. This forces the executable
blocks to be rather small. Additionally, there are several
optimization opportunities that exist in the final assembly code that
are not available in the assembly generation level in CompCert. This
is because there are several pseudo-instructions which capture more
sophisticated behavior than a single assembly instruction. These
pseudo-instructions are expanded into their final instructions in the
pretty printing routines and are consequently not visible to our
optimizer. We feel that with additional time it would be possible to
expand these instructions in the optimizer allowing them to be
optimized. It is unclear how this might affect the overall proofs.

\paragraph{Treatment of flags: } 
The flags follow x86 semantics\footnote{x86 semantics as defined by
  Intel} as closely as possible.\footnote{CompCert ignores some flags
  and conflates others, providing only one register bit for two
  separate flags.}  By following the Intel specification, the
validator is more restrictive than, but not in conflict with, the
CompCert semantics which declares the flags to be undefined after most
arithmetic instructions.  Because this work defines normalization as
an operation that maintains arithmetic equivalence, but not necessarily
the flag value's equivalence, it is not possible to normalize the
expressions when comparing flag values.  Instead, a straight forward
syntactic equality is used which could result in unnecessary rejection
of optimized code.

\section{False Starts}
In general, the false starts give a solid lessons learned of
explicitly identifying and verifying the assumptions early.


\paragraph{Direct addition to the x86 ASM stage: }
The first attempt was to add peephole optimization directly to the ASM
stage in CompCert.  After much wrestling with existing proofs, which
broke sometimes only incidentally due to the peephole optimizer, we
decided to instead add this work as a new final stage to the CompCert
pipeline.  As a result there were minor, high level, additions in
Compiler.v to both add the optimization pass and apply an (admitted)
theorem for the end-to-end proof.

\paragraph{Avoidance of Alias Analysis: }
Our canonical optimizations were instruction weakening and removing
sequential redundant loads, neither of which require alias analysis.
Due to pseudo instructions there were zero redundant loads visible to
the peephole optimizer; coupled with the generally larger scope of
instruction weakening, this forced us to reconsider our intended
showcase optimization and include proper alias analysis.

\paragraph{Use of {\tt addrmode} for address information}
Even late into development, CompCert's {\tt addrmode} was used to represent
memory addressing under the implicit assumption {\tt addrmode} equality would 
correctly identify aliasing.  Briefly, {\tt addrmode}s are
insufficient because they capture a base in terms of a particular register
and not the contents of the register at the time the address was used.
Replacing this dependence was the source of significant code and proof
reworking.


\section{Impact of the Optimizer}
In our test code (consisting of benchmarks from the ``Computer
Language Shoot-out'') we remove 29 assembly instructions when
compiling ~1000 lines of C code that result in 2416 assembly
instructions.  All the instruction removals are due to the redundant
load optimization. 

Generally speaking the one currently functioning optimization only
impacts functions with multiple arguments. In the benchmarks tested,
the optimization generally appears in code that is either early in
{\tt main()} or in functions which are not part of a tight
loop. Consequently, the optimization has almost no impact. However,
for the {\tt recursive.c} test, the optimization is applied in the
implementation of Ackerman's function. In this particular case, where
the optimization happens to fall in the argument handling code for a
heavily recursive function, we see a speedup of approximately 5\%
versus the unoptimized code. While this is certainly not a dramatic
result, it does show that these types of optimizations can improve the
performance of x86 code generated by CompCert. A more fully developed
suite of optimizations may have significant impact in a variety of
programs.

\section{Conclusion}

We consider these results to be a reasonable ``first try.'' We
dramatically underestimated the scope of the project and failed to
understand the implications of some of our design
decisions. Certainly, this is our first serious attempt at development
within a theorem prover and there were bound to be some hard
lessons. We spent considerable time focusing on the engineering of an
optimizer and learning after the fact that our solutions were not well
suited to verification. In retrospect, if we had focused more on
pre-development design, development of an overall proof strategy, and
the implementation of a framework that better connects to CompCert we
would have perhaps not had a working optimizer, but a more solid
foundation for completion of the overall project.

\clearpage
\appendix
\section{Installation Instructions}
To obtain the code simply git clone the repository.  From here on we
assume you cloned into the {\tt ccomp-ph} directory.

\begin{verbatim}
$ git clone git://github.com/TomMD/CompCert-Peep-hole-Optimizer.git ccomp-ph
\end{verbatim}

The source can be configured, compiled, and installed in the normal
manner (we recommend using Coq 8.2pl2):

\begin{verbatim}
$ cd ccomp-ph/compcert-1.8
$ ./configure ia32-linux && make all && sudo make install
\end{verbatim}

Compiling and testing, be it generating assembly or building an
executable, can proceed as normal:

\begin{verbatim}
$ cd comp-ph/benchmarks/clso
$ ccomp -S *.c
$ ls *.s
\end{verbatim}

\section{Code Layout}
Within the {\tt ccomp-ph/compcert-1.8} directory the following files
were modified and/or added:

\begin{itemize}
\item {\tt ia32/Peephole.v} -- contains the symbolic execution,
  validator, and call-out to the ML optimizer.
\item {\tt ia32/Peepholeproof.v} -- contains proofs about
  properties of the operations in {\tt Peephole.v}.
\item {\tt ia32/Peepholeaux.ml\{,i\}} -- the ML peephole
  optimizer with basic, load and store centric, optimizations.
\item {\tt driver/Compiler.v} -- Modified to call our
  optimization pass and use the (admitted) proof of correctness.
\item {\tt ia32/Asm.v} -- added equality tests for various data
  types and added a {\tt NOP} instruction.
\item {\tt ia32/PrintAsm.ml} -- Modified to emit {\tt NOP} instructions.
\item {\tt extraction/extraction.v} -- modified in the obvious
  manner to enable calling the ML optimizer.
\end{itemize}

\end{document}
